---
title: "Long Document Modeling"
collection: research_nlp
permalink: /research/nlp/long_document_modeling
excerpt: "This project aim to improve Transformer for long document modeling task using Graph Neural Network. We observed that Transformer is equivalent to the Graph Attention Network (GAT) applied to a fully connected graph of words. So, we greatly reduced the memory consumption via graph sparsification and attained a satisfactory level of accuracy on IMDB dataset. Inspired by Graph Pooling, we proposed a text pooling module called Sequential Edge Pooling to extend the receptive field and capture long-range dependency. Compared to traditional pooling methods, SEP could preserve the semantic structure and produce an interpretable "parse tree". Our method is competitive with the state-of-the-art methods on IMDB."
start_date: Aug. 2019
end_date: Sep. 2019
date: Sep. 2019
selected: true
---

This project aim to improve Transformer for long document modeling task using Graph Neural Network.

 We observed that Transformer is equivalent to the Graph Attention Network (GAT) applied to a fully connected graph of words. So, we greatly reduced the memory consumption via graph sparsification and attained a satisfactory level of accuracy on IMDB dataset. Inspired by Graph Pooling, we proposed a text pooling module called Sequential Edge Pooling to extend the receptive field and capture long-range dependency. Compared to traditional pooling methods, SEP could preserve the semantic structure and produce an interpretable "parse tree". Our method is competitive with the state-of-the-art methods on IMDB.

This project is led by me and supervised by Ziniu Hu, Prof. Wei Wang and Prof. Yizhou Sun. Yunsheng Bai and Haonan Wang also helped a lot during my internship. Many thanks to them!

