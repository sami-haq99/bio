---
title: "Improving BERT-based Language Understanding"
collection: research_nlp
permalink: /research/nlp/improving_bert
excerpt: "This project aims to improve BERT (and its variants such as RoBERTa) for natural language understanding tasks. We used well-trained NMT models to provide additional semantic information and to improve the performance of BERT on GLUE dataset. By feeding the contextualized word embeddings generated by Transformer-based NMT models into BERT, we improved accuracy by 1.1% on the SST-2 dataset and 1.5% on the RTE dataset. Now we are applying dual inference framework to sentence classification tasks by training a language model conditioned on the label of a sentence."
start_date: Oct. 2019
date: Jan. 2020
selected: true
---

This project aims to improve BERT (and its variants such as RoBERTa) for natural language understanding tasks.

We used well-trained NMT models to provide additional semantic information and to improve the performance of BERT on GLUE dataset. By feeding the contextualized word embeddings generated by Transformer-based NMT models into BERT, we improved accuracy by 1.1% on the SST-2 dataset and 1.5% on the RTE dataset. Now we are applying dual inference framework to sentence classification tasks by training a language model conditioned on the label of a sentence.

This project is led by me and supervised by Dr. Yingce Xia and Dr. Tao Qin. Many thanks to their help!

